<!doctype html><html lang=zh-cn><head><title>强化学习(1) // NyanCの錬金工坊</title><link rel="shortcut icon" href=/images/github.png><meta charset=utf-8><meta name=generator content="Hugo 0.81.0"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="NyanC"><meta name=description content><link rel=stylesheet href=https://blog.nyanco.top/css/main.min.8bd5320ea12af3d7b52787813b9ec3f3f32caa99020e75c8196b233eedafc70e.css><meta name=twitter:card content="summary"><meta name=twitter:title content="强化学习(1)"><meta name=twitter:description content="强化学习学习笔记"><meta property="og:title" content="强化学习(1)"><meta property="og:description" content="强化学习学习笔记"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.nyanco.top/posts/rl-1/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-17T15:50:01+08:00"><meta property="article:modified_time" content="2022-10-20T08:08:38+00:00"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css></head><body><header class=app-header><a href=https://blog.nyanco.top><img class=app-header-avatar src=/images/avatar.jpg alt=NyanC></a><h1>NyanCの錬金工坊</h1><nav class=app-header-menu><a class=app-header-menu-item href=/>Home</a>
-
<a class=app-header-menu-item href=/tags/>Tags</a>
-
<a class=app-header-menu-item href=/about/>About</a>
-
<a class=app-header-menu-item href=/friends/>Friends</a></nav><p>At least I was a Big Dreamer.</p><div class=app-header-social><a href=https://github.com/Nayaco target=_blank rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github"><title>Github</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=/index.xml target=_blank rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-rss"><title>RSS</title><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></div></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>强化学习(1)</h1><div class=post-meta><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Oct 17, 2022</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>1 min read</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7.01" y2="7"/></svg><a class=tag href=https://blog.nyanco.top/tags/ml/>ML</a>
<a class=tag href=https://blog.nyanco.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></div></div></header><div class=post-content><p><img src=/images/qdn-1.png alt></p><h2 id=动作价值函数基于价值>动作价值函数(基于价值)</h2><ul><li>Cumulative Discounted Future Reward</li></ul><p>$$
U_t = \sum_{n=0}^{&mldr;} \gamma^{n} R_{t+n}
$$</p><p>Return UT 依赖于之后的状态S动作A</p><ul><li>动作价值函数\(Q_\pi\)</li></ul><p>$\pi$是策略, 于是动作价值函数可以表达为</p><p>$$Q_\pi(s_t, a_t)=E[U_t|S_t=s_t, A_t=a_t]$$</p><p>取最大得到最优动作价值函数(optimal action-value function):</p><p>$$Q^*(s_t, a_t)=\max_{\pi}Q_\pi(s_t, a_t)$$</p><ul><li>最优动作\(a^<em>=\argmax_aQ^</em>(s_t,a_t)\)</li></ul><p>基于动作价值函数训练Q-function令\(Q(s,a;w)\approx Q^*(s,a)\)</p><ul><li>训练QDN: TD(Temporal Difference Learning)</li></ul><p>TD算法核心\(T_{estimateOverall} \approx T_{realAtoMid} + T_{estimateMidtoB}\)</p><p>$$\to Q(s_t,a_t;w) \approx r_t + \gamma Q(s_{t+1},a_{t+1};w)$$</p><p>其中\(r_t\)为当前时刻价值的ground truth</p><p>该推论可通过折扣回报的展开得到, 公式来源于Q的自治条件</p><p>TD的目标是让\(Q(s_t,a_t;w)\)无限接近于公式的后半部分</p><p>$$
y_r = r_t + \gamma Q(s_{t+1},a_{t+1};w)\<br>L_{t} = \frac{1}{2}[Q(s_t,a_t;w) - y_t]^2\<br>w_{t+1} = w_{t} - \alpha \cdot \frac{\partial L_t}{\partial w}|_{w=w_t}
$$</p><p>通过梯度下降就可以学习w</p><h2 id=状态价值函数基于策略>状态价值函数(基于策略)</h2><ul><li>策略函数
\(\pi(a|s)\)输出为当前状态选择a的概率</li></ul><p>参数化策略函数\(\pi(a|s;\theta)\) \(\theta\)是可训练参数</p><ul><li>状态价值函数</li></ul><p>$$V_\pi(s_t)=E_A[Q_\pi(s_t,A)]=\sum_a \pi(a|s_t) \cdot Q_\pi(s_t,a), \space (A \sim \pi(\cdot|s_t))$$</p><p>$$V_\pi(s_t;\theta)=\sum_a \pi(a|s_t;\theta) \cdot Q_\pi(s_t,a)$$</p><p>目标是最大化\(J(\theta)=E_S[V_pi(S;\theta)]\), 这里采用策略梯度下降(不严谨推导&ndash;ShusenWang)</p><ul><li><em>采样s</em></li><li>\(\theta \larr \theta + \beta \frac{\partial V(s;\theta)}{\partial \theta}\)</li></ul><p>$$\frac{\partial V(s;\theta)}{\partial \theta} = \sum_a\frac{\partial \pi(a|s;\theta)}{\partial \theta}\cdot Q_\pi(s,a)=\sum_a\pi(a|s;\theta)\frac{\partial log\pi(a|s;\theta)}{\partial \theta}\cdot Q_\pi(s,a)=E_A[\frac{\partial log\pi(A|s;\theta)}{\partial \theta}\cdot Q_\pi(s,A)], Q_\pi \space independent \space with \space \theta$$</p><p>对于连续变量采用蒙特卡罗近似近似上述期望</p><p><img src=/images/qdn-2.png alt></p><p>Q的近似:</p><ul><li><p>Reinforce</p></li><li><p>Actor-Critic(AC同时近似了V和Q, 实际上是两个神经网络)</p></li></ul><h2 id=小结>小结</h2><p>强化学习的实际上是通过近似价值函数来优化当前动作以取得期望收益的最大化, 其中又分为基于价值和基于策略的方法. 价值方法使用动作价值函数Q, 训练Q的权值矩阵w, 令价值函数近似ground truth, 选取价值最大的动作; 而策略方法使用状态价值函数V, 训练$\pi$的权值矩阵$\theta$, 通过令$V_\pi(s;\theta)$近似ground truth(即最大化), 优化策略本身.</p></div><div class=post-footer></div></article><link rel=stylesheet href=https://yandex.st/highlightjs/8.0/styles/default.min.css><script src=https://yandex.st/highlightjs/8.0/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script><style>.hljs{background:#eeeaea}</style><div class=post-comment><div id=vcomments class=app-comment></div><script src=//cdn1.lncld.net/static/js/3.0.4/av-min.js></script><script src=//unpkg.com/valine/dist/Valine.min.js></script><script type=text/javascript>new Valine({el:'#vcomments',appId:'ri1SLKJpbyyESG14uQHiwDtc-gzGzoHsz',appKey:'2MjdhHpp1yusSckOo8x7YVli',notify:'false',verify:'false',avatar:'monsterid',placeholder:'Comment Here...',visitor:'true'})</script><link rel=stylesheet href=https://blog.nyanco.top/css/valine.css></div><hr style="filter:alpha (opacity=0,finishopacity=100,style=1 )" width=100% color=#bbb cb 10 size=3><div class="footer container-xl width-full p-responsive"><div class="position-relative d-flex flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between flex-sm-items-center pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light"><ul class="list-style-none d-flex flex-wrap col-12 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0" style=list-style:none><li class="mr-3 mr-lg-0"><a aria-label=Homepage title=GitHub class="footer-octicon d-none d-lg-block mr-lg-4" style=margin-right:.5em href=https://blog.nyanco.top><svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" width="24"><path fill-rule="evenodd" d="M8 0C3.58.0.0 3.58.0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38.0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95.0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12.0.0.67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15.0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48.0 1.07-.01 1.93-.01 2.2.0.21.15.46.55.38A8.013 8.013.0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a>Theme by <a href=https://github.com/vaga/hugo-theme-m10c>m10c</a> & <a href=https://github.com/Nayaco/nyan-blog>nyan-blog</a></li></ul></div><div class="d-flex flex-justify-center pb-6"><span class="f6 text-gray-light"></span></div></div></main><script src=https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js></script></body></html>